# -*- coding: utf-8 -*-
"""PerceptrÃ³n

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FPJGb1vsP27sMYuzEK78Zfk1dBWe5CEw
"""

# perceptron_tests.py
import numpy as np

# --------------------
# Activaciones y derivadas
# --------------------
def linear(x):
    return x

def d_linear(x):
    return np.ones_like(x)

def step(x):
    return (x >= 0).astype(float)

def d_step(x):
    # derivada no definida en 0; para entrenamiento basado en gradiente usaremos 0
    return np.zeros_like(x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def d_sigmoid(x):
    s = sigmoid(x)
    return s * (1 - s)

def relu(x):
    return np.maximum(0, x)

def d_relu(x):
    return (x > 0).astype(float)

def tanh_act(x):
    return np.tanh(x)

def d_tanh(x):
    return 1 - np.tanh(x) ** 2

def softmax(x):
    # x shape: (n_samples, n_classes)
    z = x - np.max(x, axis=1, keepdims=True)
    e = np.exp(z)
    return e / np.sum(e, axis=1, keepdims=True)

# --------------------
# Perceptron/Neuron class
# --------------------
class Perceptron:
    def __init__(self, n_inputs, activation='step', lr=0.1, n_outputs=1, seed=42):
        np.random.seed(seed)
        self.n_inputs = n_inputs
        self.lr = lr
        self.n_outputs = n_outputs
        self.W = np.random.randn(n_inputs + 1, n_outputs) * 0.01  # +1 bias
        self.set_activation(activation)

    def set_activation(self, name):
        name = name.lower()
        if name == 'linear':
            self.act = linear; self.dact = d_linear; self.type='reg'
        elif name == 'step':
            self.act = step; self.dact = d_step; self.type='perceptron'
        elif name == 'sigmoid':
            self.act = sigmoid; self.dact = d_sigmoid; self.type='prob'
        elif name == 'relu':
            self.act = relu; self.dact = d_relu; self.type='reg'
        elif name == 'tanh':
            self.act = tanh_act; self.dact = d_tanh; self.type='prob'
        elif name == 'softmax':
            self.act = softmax; self.dact = None; self.type='softmax'
        else:
            raise ValueError("Unknown activation")

    def forward(self, X):
        # X: (n_samples, n_inputs)
        Xb = np.hstack([X, np.ones((X.shape[0],1))])  # bias column
        z = Xb.dot(self.W)  # shape (n_samples, n_outputs)
        if self.type == 'softmax':
            return self.act(z), z
        else:
            return self.act(z), z

    def predict(self, X):
        y_hat, z = self.forward(X)
        if self.type == 'softmax':
            return np.argmax(y_hat, axis=1)
        if self.n_outputs == 1:
            if self.act == sigmoid:
                return (y_hat[:,0] >= 0.5).astype(int)
            elif self.act == step:
                return y_hat[:,0].astype(int)
            else:
                return (y_hat[:,0] >= 0.5).astype(int)
        else:
            return np.argmax(y_hat, axis=1)

    def fit(self, X, y, epochs=100, verbose=False):
        """
        X: (n_samples, n_inputs)
        y: if n_outputs==1 -> shape (n_samples,) with 0/1
           if n_outputs>1 -> shape (n_samples,) with class indices 0..C-1
        """
        Xb = np.hstack([X, np.ones((X.shape[0],1))])
        n = X.shape[0]

        if self.type == 'perceptron':
            # classic perceptron learning rule (for step)
            for epoch in range(epochs):
                errors = 0
                for i in range(n):
                    xi = Xb[i:i+1]        # (1, n_inputs+1)
                    zi = xi.dot(self.W)   # (1, n_outputs)
                    out = self.act(zi)    # step output
                    target = y[i] if self.n_outputs==1 else self._one_hot(y[i])
                    err = target - out.ravel()
                    if np.any(err != 0):
                        errors += 1
                        self.W += self.lr * xi.T.dot(err.reshape(1, -1))
                if verbose:
                    print(f"Epoch {epoch+1}/{epochs} errors: {errors}")
                if errors == 0:
                    break
        elif self.type == 'softmax':
            # cross-entropy + gradient descent
            C = self.n_outputs
            Y = np.zeros((n, C))
            Y[np.arange(n), y] = 1
            for epoch in range(epochs):
                z = Xb.dot(self.W)                 # (n, C)
                p = softmax(z)                     # (n, C)
                grad = (Xb.T.dot(p - Y)) / n       # (n_inputs+1, C)
                self.W -= self.lr * grad
                if verbose and epoch % (epochs//10 + 1) == 0:
                    loss = -np.mean(np.sum(Y * np.log(p + 1e-12), axis=1))
                    print(f"Epoch {epoch+1}/{epochs} loss: {loss:.4f}")
        else:
            # differentiable activations: use mean squared error gradient descent (simple)
            # for binary output we assume n_outputs==1 and y in {0,1}
            if self.n_outputs == 1:
                Y = y.reshape(-1,1)
            else:
                # one-hot
                Y = np.zeros((n, self.n_outputs))
                Y[np.arange(n), y] = 1
            for epoch in range(epochs):
                z = Xb.dot(self.W)        # (n, out)
                a = self.act(z)           # (n, out)
                # Use MSE loss: L = 1/2 mean (a - Y)^2 -> gradient w.r.t z = (a - Y) * dact(z)
                delta = (a - Y) * self.dact(z)
                grad = (Xb.T.dot(delta)) / n
                self.W -= self.lr * grad
                if verbose and epoch % (epochs//10 + 1) == 0:
                    loss = 0.5 * np.mean(np.sum((a - Y)**2, axis=1))
                    print(f"Epoch {epoch+1}/{epochs} mse: {loss:.4f}")

    def _one_hot(self, idx):
        v = np.zeros((1, self.n_outputs))
        v[0, idx] = 1
        return v


# --------------------
# PRUEBAS
# --------------------
def test_and():
    X = np.array([[0,0],[0,1],[1,0],[1,1]])
    y = np.array([0,0,0,1])
    p = Perceptron(2, activation='step', lr=0.1, n_outputs=1)
    p.fit(X, y, epochs=50, verbose=True)
    print("AND preds:", p.predict(X), "target:", y)

def test_or():
    X = np.array([[0,0],[0,1],[1,0],[1,1]])
    y = np.array([0,1,1,1])
    p = Perceptron(2, activation='step', lr=0.1, n_outputs=1)
    p.fit(X, y, epochs=50, verbose=True)
    print("OR preds:", p.predict(X), "target:", y)

def synthetic_binary_task(n_features=4, seed=0):
    np.random.seed(seed)
    n = 200
    X = np.random.randn(n, n_features)
    # target = sign of sum of features + some noise -> 0/1
    y = (np.sum(X[:, :2], axis=1) + 0.5 * X[:,2] > 0).astype(int)
    return X, y

def test_spam_like():
    X, y = synthetic_binary_task(n_features=6)
    p = Perceptron(X.shape[1], activation='sigmoid', lr=0.5, n_outputs=1)
    p.fit(X, y, epochs=200, verbose=True)
    preds = p.predict(X)
    acc = (preds == y).mean()
    print("Spam-like acc:", acc)

def test_weather_like():
    # small synthetic: features [temp, humidity, wind] -> rain(1)/no rain(0)
    X = np.array([[30,30,5],[25,80,3],[20,90,6],[28,40,2],[15,85,10]])
    y = np.array([0,1,1,0,1])
    p = Perceptron(3, activation='tanh', lr=0.1, n_outputs=1)
    p.fit(X, y, epochs=150, verbose=True)
    print("Weather preds:", p.predict(X), "target", y)

def test_fraud_like():
    X, y = synthetic_binary_task(n_features=8, seed=7)
    p = Perceptron(X.shape[1], activation='relu', lr=0.01, n_outputs=1)
    p.fit(X, y, epochs=300, verbose=True)
    print("Fraud-like acc:", (p.predict(X) == y).mean())

def test_academic_risk():
    # features: [attendance_rate, avg_grade, failed_courses_count]
    X = np.array([
        [0.9, 85, 0],
        [0.7, 60, 1],
        [0.4, 50, 2],
        [0.95, 90, 0],
        [0.6, 55, 1],
        [0.3, 40, 3]
    ])
    y = np.array([0,1,1,0,1,1])  # 1 = at-risk
    p = Perceptron(3, activation='sigmoid', lr=0.5, n_outputs=1)
    p.fit(X, y, epochs=200, verbose=True)
    print("Academic risk preds:", p.predict(X), "target", y)


if __name__ == "__main__":
    test_and()
    test_or()
    test_spam_like()
    test_weather_like()
    test_fraud_like()
    test_academic_risk()