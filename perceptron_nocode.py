# -*- coding: utf-8 -*-
"""Perceptron_NoCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qL3aMGC76gxjXmGEPmCn5nOjQ6PmXmh8
"""

# perceptron_tests.py
import numpy as np
import matplotlib.pyplot as plt

# --------------------
# Funciones de activación y sus derivadas
# --------------------
def lineal(x):
    return x
def d_lineal(x):
    return np.ones_like(x)

def escalon(x):
    return (x >= 0).astype(float)
def d_escalon(x):
    return np.zeros_like(x)

def sigmoide(x):
    return 1 / (1 + np.exp(-x))
def d_sigmoide(x):
    s = sigmoide(x)
    return s * (1 - s)

def relu(x):
    return np.maximum(0, x)
def d_relu(x):
    return (x > 0).astype(float)

def tangente_hip(x):
    return np.tanh(x)
def d_tangente_hip(x):
    return 1 - np.tanh(x) ** 2

def softmax(x):
    z = x - np.max(x, axis=1, keepdims=True)
    e = np.exp(z)
    return e / np.sum(e, axis=1, keepdims=True)

# --------------------
# Clase Perceptrón
# --------------------
class Perceptron:
    def __init__(self, n_entradas, activacion='escalon', tasa=0.1, n_salidas=1, semilla=42):
        np.random.seed(semilla)
        self.n_entradas = n_entradas
        self.tasa = tasa
        self.n_salidas = n_salidas
        self.W = np.random.randn(n_entradas + 1, n_salidas) * 0.01  # incluye sesgo
        self.definir_activacion(activacion)

    def definir_activacion(self, nombre):
        nombre = nombre.lower()
        if nombre == 'lineal':
            self.act = lineal; self.dact = d_lineal; self.tipo='reg'
        elif nombre == 'escalon':
            self.act = escalon; self.dact = d_escalon; self.tipo='perceptron'
        elif nombre == 'sigmoide':
            self.act = sigmoide; self.dact = d_sigmoide; self.tipo='prob'
        elif nombre == 'relu':
            self.act = relu; self.dact = d_relu; self.tipo='reg'
        elif nombre == 'tanh' or nombre == 'tangente':
            self.act = tangente_hip; self.dact = d_tangente_hip; self.tipo='prob'
        elif nombre == 'softmax':
            self.act = softmax; self.dact = None; self.tipo='softmax'
        else:
            raise ValueError("Función de activación no reconocida")

    def propagar(self, X):
        Xb = np.hstack([X, np.ones((X.shape[0],1))])
        z = Xb.dot(self.W)
        if self.tipo == 'softmax':
            return self.act(z), z
        else:
            return self.act(z), z

    def predecir(self, X):
        y_hat, z = self.propagar(X)
        if self.tipo == 'softmax':
            return np.argmax(y_hat, axis=1)
        if self.n_salidas == 1:
            if self.act == sigmoide:
                return (y_hat[:,0] >= 0.5).astype(int)
            elif self.act == escalon:
                return y_hat[:,0].astype(int)
            else:
                return (y_hat[:,0] >= 0.5).astype(int)
        else:
            return np.argmax(y_hat, axis=1)

    def entrenar(self, X, y, epocas=100, verbose=False):
        Xb = np.hstack([X, np.ones((X.shape[0],1))])
        n = X.shape[0]
        historial = []

        if self.tipo == 'perceptron':
            for epoca in range(epocas):
                errores = 0
                for i in range(n):
                    xi = Xb[i:i+1]
                    zi = xi.dot(self.W)
                    out = self.act(zi)
                    objetivo = y[i] if self.n_salidas==1 else self._one_hot(y[i])
                    err = objetivo - out.ravel()
                    if np.any(err != 0):
                        errores += 1
                        self.W += self.tasa * xi.T.dot(err.reshape(1, -1))
                historial.append(errores)
                if verbose:
                    print(f"Época {epoca+1}: errores={errores}")
                if errores == 0:
                    break

        elif self.tipo == 'softmax':
            C = self.n_salidas
            Y = np.zeros((n, C))
            Y[np.arange(n), y] = 1
            for epoca in range(epocas):
                z = Xb.dot(self.W)
                p = softmax(z)
                grad = (Xb.T.dot(p - Y)) / n
                self.W -= self.tasa * grad
                perdida = -np.mean(np.sum(Y * np.log(p + 1e-12), axis=1))
                historial.append(perdida)
                if verbose and epoca % (epocas//10 + 1) == 0:
                    print(f"Época {epoca+1}: pérdida={perdida:.4f}")

        else:
            if self.n_salidas == 1:
                Y = y.reshape(-1,1)
            else:
                Y = np.zeros((n, self.n_salidas))
                Y[np.arange(n), y] = 1
            for epoca in range(epocas):
                z = Xb.dot(self.W)
                a = self.act(z)
                delta = (a - Y) * self.dact(z)
                grad = (Xb.T.dot(delta)) / n
                self.W -= self.tasa * grad
                perdida = 0.5 * np.mean(np.sum((a - Y)**2, axis=1))
                historial.append(perdida)
                if verbose and epoca % (epocas//10 + 1) == 0:
                    print(f"Época {epoca+1}: MSE={perdida:.4f}")
        return historial

    def _one_hot(self, idx):
        v = np.zeros((1, self.n_salidas))
        v[0, idx] = 1
        return v

# --------------------
# Funciones de prueba
# --------------------
def graficar(historial, titulo, ylabel="Error/Pérdida"):
    plt.plot(range(1, len(historial)+1), historial, marker='o')
    plt.title(titulo)
    plt.xlabel("Épocas")
    plt.ylabel(ylabel)
    plt.grid(True)
    plt.show()

def prueba_and():
    X = np.array([[0,0],[0,1],[1,0],[1,1]])
    y = np.array([0,0,0,1])
    p = Perceptron(2, activacion='escalon', tasa=0.1, n_salidas=1)
    historial = p.entrenar(X, y, epocas=50, verbose=True)
    print("Predicciones AND:", p.predecir(X), "Objetivo:", y)
    graficar(historial, "Entrenamiento Perceptrón - AND lógico", "Errores")

def prueba_or():
    X = np.array([[0,0],[0,1],[1,0],[1,1]])
    y = np.array([0,1,1,1])
    p = Perceptron(2, activacion='escalon', tasa=0.1, n_salidas=1)
    historial = p.entrenar(X, y, epocas=50, verbose=True)
    print("Predicciones OR:", p.predecir(X), "Objetivo:", y)
    graficar(historial, "Entrenamiento Perceptrón - OR lógico", "Errores")

def tarea_binaria_sintetica(n_features=4, semilla=0):
    np.random.seed(semilla)
    n = 200
    X = np.random.randn(n, n_features)
    y = (np.sum(X[:, :2], axis=1) + 0.5 * X[:,2] > 0).astype(int)
    return X, y

def prueba_spam():
    X, y = tarea_binaria_sintetica(n_features=6)
    p = Perceptron(X.shape[1], activacion='sigmoide', tasa=0.5, n_salidas=1)
    historial = p.entrenar(X, y, epocas=200, verbose=True)
    preds = p.predecir(X)
    acc = (preds == y).mean()
    print("Exactitud Spam (sintético):", acc)
    graficar(historial, "Entrenamiento Perceptrón - Spam", "MSE")

def prueba_clima():
    X = np.array([[30,30,5],[25,80,3],[20,90,6],[28,40,2],[15,85,10]])
    y = np.array([0,1,1,0,1])
    p = Perceptron(3, activacion='tanh', tasa=0.1, n_salidas=1)
    historial = p.entrenar(X, y, epocas=150, verbose=True)
    print("Predicciones Clima:", p.predecir(X), "Objetivo:", y)
    graficar(historial, "Entrenamiento Perceptrón - Clima", "MSE")

def prueba_fraude():
    X, y = tarea_binaria_sintetica(n_features=8, semilla=7)
    p = Perceptron(X.shape[1], activacion='relu', tasa=0.01, n_salidas=1)
    historial = p.entrenar(X, y, epocas=300, verbose=True)
    print("Exactitud Fraude:", (p.predecir(X) == y).mean())
    graficar(historial, "Entrenamiento Perceptrón - Fraude", "MSE")

def prueba_riesgo_academico():
    X = np.array([
        [0.9, 85, 0],
        [0.7, 60, 1],
        [0.4, 50, 2],
        [0.95, 90, 0],
        [0.6, 55, 1],
        [0.3, 40, 3]
    ])
    y = np.array([0,1,1,0,1,1])
    p = Perceptron(3, activacion='sigmoide', tasa=0.5, n_salidas=1)
    historial = p.entrenar(X, y, epocas=200, verbose=True)
    print("Predicciones Riesgo Académico:", p.predecir(X), "Objetivo:", y)
    graficar(historial, "Entrenamiento Perceptrón - Riesgo Académico", "MSE")

# --------------------
# Main
# --------------------
if __name__ == "__main__":
    prueba_and()
    prueba_or()
    prueba_spam()
    prueba_clima()
    prueba_fraude()
    prueba_riesgo_academico()

